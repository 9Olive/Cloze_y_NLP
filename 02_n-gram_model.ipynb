{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Set up\n",
    "\n",
    "Follow [1.2 Getting Started with NLTK](https://www.nltk.org/book/ch01.html)\n",
    "\n",
    "A better reference: [NLP with Python](https://bagustris.github.io/nlp-python/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once, or as many times as you need to configure which files you want access to. \n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga the CESS_ESP corpus. Vamos a entranar nuestro modelo\n",
    "training_corpus = nltk.corpus.cess_esp.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m N_grams_freq\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m tqdm(N):\n\u001b[1;32m---> 16\u001b[0m     N_grams[n]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mngrams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtraining_corpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     N_grams_cnt[n]\u001b[38;5;241m=\u001b[39mCounter(N_grams[n])\n\u001b[0;32m     23\u001b[0m     N_grams_cnt[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_total_cnt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m(N_grams_cnt[n]\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\JDOli\\.conda\\envs\\tensorflow-general-env\\lib\\site-packages\\nltk\\corpus\\reader\\util.py:424\u001b[0m, in \u001b[0;36mConcatenatedCorpusView.iterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_piece \u001b[38;5;241m=\u001b[39m piece\n\u001b[0;32m    423\u001b[0m \u001b[38;5;66;03m# Get everything we can from this piece.\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m piece\u001b[38;5;241m.\u001b[39miterate_from(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, start_tok \u001b[38;5;241m-\u001b[39m offset))\n\u001b[0;32m    426\u001b[0m \u001b[38;5;66;03m# Update the offset table.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m piecenum \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offsets):\n",
      "File \u001b[1;32mc:\\Users\\JDOli\\.conda\\envs\\tensorflow-general-env\\lib\\site-packages\\nltk\\corpus\\reader\\util.py:306\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.iterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_toknum \u001b[38;5;241m=\u001b[39m toknum\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_blocknum \u001b[38;5;241m=\u001b[39m block_index\n\u001b[1;32m--> 306\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m, AbstractLazySequence)), (\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock reader \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() should return list or tuple.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    310\u001b[0m )\n\u001b[0;32m    311\u001b[0m num_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n",
      "File \u001b[1;32mc:\\Users\\JDOli\\.conda\\envs\\tensorflow-general-env\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:499\u001b[0m, in \u001b[0;36mSyntaxCorpusReader._read_word_block\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_word_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream):\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_sent_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32mc:\\Users\\JDOli\\.conda\\envs\\tensorflow-general-env\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:505\u001b[0m, in \u001b[0;36mSyntaxCorpusReader._read_sent_block\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_sent_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream):\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_word(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m]))\n",
      "File \u001b[1;32mc:\\Users\\JDOli\\.conda\\envs\\tensorflow-general-env\\lib\\site-packages\\nltk\\corpus\\reader\\bracket_parse.py:68\u001b[0m, in \u001b[0;36mBracketParseCorpusReader._read_block\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_blankline_block(stream)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_blocks \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munindented_paren\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Tokens start with unindented left parens.\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     toks \u001b[38;5;241m=\u001b[39m \u001b[43mread_regexp_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_re\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m^\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Strip any comments out of the tokens.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_comment_char:\n",
      "File \u001b[1;32mc:\\Users\\JDOli\\.conda\\envs\\tensorflow-general-env\\lib\\site-packages\\nltk\\corpus\\reader\\util.py:658\u001b[0m, in \u001b[0;36mread_regexp_block\u001b[1;34m(stream, start_re, end_re)\u001b[0m\n\u001b[0;32m    656\u001b[0m lines \u001b[38;5;241m=\u001b[39m [line]\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 658\u001b[0m     oldpos \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m     line \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# End of file:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JDOli\\.conda\\envs\\tensorflow-general-env\\lib\\site-packages\\nltk\\data.py:1303\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.tell\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewind_checkpoint)\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_char_seek_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewind_numchars, est_bytes)\n\u001b[1;32m-> 1303\u001b[0m filepos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;66;03m# Sanity check\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDEBUG:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hagas los N-grams, por N=[2,3,4,5]\n",
    "N=[2,3,4,5]\n",
    "N_grams={\n",
    "    1:[],\n",
    "    2:[],\n",
    "    3:[],\n",
    "    4:[],\n",
    "    5:[]\n",
    "}\n",
    "\n",
    "# Tecnicamente, no necesitamos declarar las listas\n",
    "N_grams_cnt={}\n",
    "N_grams_freq={}\n",
    "\n",
    "for n in tqdm(N):\n",
    "    N_grams[n]=list(\n",
    "        ngrams(\n",
    "            training_corpus,\n",
    "            n\n",
    "        )\n",
    "    )\n",
    "    N_grams_cnt[n]=Counter(N_grams[n])\n",
    "    N_grams_cnt[f\"{n}_total_cnt\"]=sum(N_grams_cnt[n].values())\n",
    "    N_grams_freq[n]={\n",
    "        ngram: count / N_grams_cnt[f\"{n}_total_cnt\"] for ngram, count in N_grams_cnt[n].items()\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [13:01<00:00, 195.31s/it]\n"
     ]
    }
   ],
   "source": [
    "N_gram_mle={}\n",
    "N_gram_mle_str={}\n",
    "\n",
    "for n in tqdm(N):\n",
    "    N_gram_cache={} # no es el mismo cache del paper\n",
    "    N_gram_mle[n]={} # para gana los \n",
    "    N_gram_mle_str[n]={}\n",
    "    for gram in N_grams[n]:\n",
    "        if gram[0] in N_gram_cache.keys():\n",
    "            tkn_n_gram_cnt=N_gram_cache[gram[0]]\n",
    "        else:\n",
    "            grams_with_matching_first_word = [index for index, tup in enumerate(N_grams[n]) if gram[0]==tup[0]] # Gana los n-grams que lo tiene la primera palabra misma.\n",
    "            tkn_n_gram_cnt = N_gram_cache[gram[0]] = len(grams_with_matching_first_word) # contar los iguales\n",
    "        gram_str=\" \"\n",
    "        gram_str=gram_str.join(gram)\n",
    "        if n == 1:\n",
    "            N_gram_mle[n][gram]=round(N_grams_cnt[n][gram]/N_grams_cnt[f\"{n}_total_cnt\"], 5)\n",
    "            N_gram_mle_str[n][gram_str]=round(N_grams_cnt[n][gram]/N_grams_cnt[f\"{n}_total_cnt\"], 5)\n",
    "        else:\n",
    "            N_gram_mle[n][gram]=round(N_grams_cnt[n][gram]/tkn_n_gram_cnt, 5) # calcular MLE\n",
    "            N_gram_mle_str[n][gram_str]=round(N_grams_cnt[n][gram]/tkn_n_gram_cnt, 5) # calcular MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"n_gram_models.json\", \"w\") as f:\n",
    "    json.dump(N_gram_mle_str, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_test\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_data_sentences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_test=pd.read_csv(\"training_data_sentences.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-general-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
