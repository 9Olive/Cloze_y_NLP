{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Human Reading Prediction\n",
    "\n",
    "The study of eye movement is of great interest to neuroscience, since they reflect cognitive processes that underlie visual tasks, in particularly reading. An important variable for determining these movements is called Predictibility. That probably needs a better name. This, Predictability, reprents the prediction that one is making about the coming word during reading. In fields such as Neurolinguistics, this Predictibility variable is not estimated from a part of the text but from responses from other reading filling in the word that follows given a the same context. \n",
    "\n",
    "In parallel, the field of NLP has estimated this type of prediction in an automatic manner as some of their goals. A simple but at the same time successful example includes n-grams. Where the probability to predict a word is constructed from the appearance of the context in a large corpus of text, that represents the knowledge of the language that the reader has.  \n",
    "\n",
    "This model is able to extend the utility of recently read text for realizing such probabilities (cache n-gram). In this work it is proposed to estimate the predictability of a word in an automatic form using parts from distinct variants of the language models. For our dataset we show that the new automatic predictibility is equally as effective as the predictibility of human explained eye movements, is much better to understand, cheaper and more rapid to obtain as it does not require experiments that involve a great count of people. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Set up\n",
    "\n",
    "Follow [1.2 Getting Started with NLTK](https://www.nltk.org/book/ch01.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run once, or as many times as you need to configure which files you want access to. \n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import cess_esp as cess\n",
    "from nltk.corpus import spanish_grammars as sg\n",
    "from nltk import UnigramTagger as ut\n",
    "from nltk import BigramTagger as bt\n",
    "from nltk.util import ngrams\n",
    "import nltk, re, pprint\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Packages that are useful\n",
    "\n",
    "Probablement, conoces [numpy](https://numpy.org/) y [pandas](https://pandas.pydata.org/). Al menos que lo dos estan familar a vos. Por las dudas, lo pongo hiperv√≠nculo. \n",
    "\n",
    "[os](https://docs.python.org/3/library/os.html) es muy util para explorar carpetas en tu computer\n",
    "\n",
    "[tqdm](https://tqdm.github.io/) es muy util para ver cuando un proceso esta cargando. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import walk \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\JDOli\\AppData\\Local\\Temp\\ipykernel_3652\\4069938276.py:7: DtypeWarning: Columns (95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp=pd.read_csv(f\"{filepath}/{file}\", delimiter=\";\")\n",
      "1it [00:22, 22.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2586948, 140)\n"
     ]
    }
   ],
   "source": [
    "filepath=\"../data/LMM-CBP/csv_in\"\n",
    "data=[]\n",
    "\n",
    "# Va a caminar en las carpetas para cargar los files\n",
    "for root, dirs, files in tqdm(walk(filepath)):\n",
    "    for file in files:\n",
    "        temp=pd.read_csv(f\"{filepath}/{file}\", delimiter=\";\")\n",
    "        data.append(temp)\n",
    "\n",
    "del temp\n",
    "\n",
    "df=pd.concat(data)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suj_id</th>\n",
       "      <th>n_orac</th>\n",
       "      <th>palnum</th>\n",
       "      <th>tipo</th>\n",
       "      <th>pred</th>\n",
       "      <th>freq</th>\n",
       "      <th>length</th>\n",
       "      <th>MaxJump</th>\n",
       "      <th>bad_epoch</th>\n",
       "      <th>stopword</th>\n",
       "      <th>...</th>\n",
       "      <th>E119</th>\n",
       "      <th>E120</th>\n",
       "      <th>E121</th>\n",
       "      <th>E122</th>\n",
       "      <th>E123</th>\n",
       "      <th>E124</th>\n",
       "      <th>E125</th>\n",
       "      <th>E126</th>\n",
       "      <th>E127</th>\n",
       "      <th>E128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586948e+06</td>\n",
       "      <td>2.586947e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.450000e+01</td>\n",
       "      <td>9.485284e+01</td>\n",
       "      <td>4.375697e+00</td>\n",
       "      <td>1.016722e+00</td>\n",
       "      <td>-2.119133e-01</td>\n",
       "      <td>4.377367e+04</td>\n",
       "      <td>4.341137e+00</td>\n",
       "      <td>3.199554e-01</td>\n",
       "      <td>6.005733e-01</td>\n",
       "      <td>4.760312e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.378470e-02</td>\n",
       "      <td>-3.066079e-03</td>\n",
       "      <td>-4.883013e-02</td>\n",
       "      <td>1.872346e-02</td>\n",
       "      <td>-4.121687e-02</td>\n",
       "      <td>-4.585046e-02</td>\n",
       "      <td>4.162723e-02</td>\n",
       "      <td>1.097454e-01</td>\n",
       "      <td>1.410374e-01</td>\n",
       "      <td>1.750807e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.077749e+00</td>\n",
       "      <td>4.925550e+01</td>\n",
       "      <td>2.373298e+00</td>\n",
       "      <td>8.517471e-01</td>\n",
       "      <td>9.609049e-01</td>\n",
       "      <td>6.940815e+04</td>\n",
       "      <td>2.360397e+00</td>\n",
       "      <td>1.217291e+00</td>\n",
       "      <td>4.897807e-01</td>\n",
       "      <td>4.994253e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.199427e+01</td>\n",
       "      <td>1.287864e+01</td>\n",
       "      <td>1.201152e+01</td>\n",
       "      <td>1.230401e+01</td>\n",
       "      <td>1.280992e+01</td>\n",
       "      <td>1.357826e+01</td>\n",
       "      <td>1.460180e+01</td>\n",
       "      <td>1.268019e+01</td>\n",
       "      <td>1.212173e+01</td>\n",
       "      <td>1.277105e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.255300e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.808102e+02</td>\n",
       "      <td>-7.236652e+02</td>\n",
       "      <td>-7.258200e+02</td>\n",
       "      <td>-7.262158e+02</td>\n",
       "      <td>-7.530132e+02</td>\n",
       "      <td>-7.490185e+02</td>\n",
       "      <td>-7.527536e+02</td>\n",
       "      <td>-7.541962e+02</td>\n",
       "      <td>-7.518157e+02</td>\n",
       "      <td>-7.520677e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.750000e+00</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.255300e+00</td>\n",
       "      <td>1.940000e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.574130e+00</td>\n",
       "      <td>-6.429716e+00</td>\n",
       "      <td>-6.499453e+00</td>\n",
       "      <td>-6.574852e+00</td>\n",
       "      <td>-6.698232e+00</td>\n",
       "      <td>-6.586955e+00</td>\n",
       "      <td>-7.274955e+00</td>\n",
       "      <td>-6.762504e+00</td>\n",
       "      <td>-6.425458e+00</td>\n",
       "      <td>-6.377016e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.450000e+01</td>\n",
       "      <td>9.300000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-3.010300e-01</td>\n",
       "      <td>2.718000e+03</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.456375e-01</td>\n",
       "      <td>-1.038615e-01</td>\n",
       "      <td>-8.447389e-02</td>\n",
       "      <td>-2.715446e-02</td>\n",
       "      <td>-3.462234e-02</td>\n",
       "      <td>-1.620925e-02</td>\n",
       "      <td>-3.477534e-03</td>\n",
       "      <td>3.852869e-02</td>\n",
       "      <td>9.089095e-02</td>\n",
       "      <td>8.002501e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.125000e+01</td>\n",
       "      <td>1.380000e+02</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>6.989700e-01</td>\n",
       "      <td>6.221400e+04</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.298971e+00</td>\n",
       "      <td>6.258519e+00</td>\n",
       "      <td>6.357286e+00</td>\n",
       "      <td>6.577438e+00</td>\n",
       "      <td>6.600107e+00</td>\n",
       "      <td>6.479055e+00</td>\n",
       "      <td>7.341053e+00</td>\n",
       "      <td>6.925016e+00</td>\n",
       "      <td>6.657640e+00</td>\n",
       "      <td>6.611491e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>1.970000e+02</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.255300e+00</td>\n",
       "      <td>2.647210e+05</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.967297e+02</td>\n",
       "      <td>1.282204e+03</td>\n",
       "      <td>1.885576e+02</td>\n",
       "      <td>2.219662e+02</td>\n",
       "      <td>1.895408e+02</td>\n",
       "      <td>3.634467e+02</td>\n",
       "      <td>2.147726e+02</td>\n",
       "      <td>1.707255e+02</td>\n",
       "      <td>1.692756e+02</td>\n",
       "      <td>2.543207e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             suj_id        n_orac        palnum          tipo          pred  \\\n",
       "count  2.586948e+06  2.586948e+06  2.586948e+06  2.586948e+06  2.586948e+06   \n",
       "mean   1.450000e+01  9.485284e+01  4.375697e+00  1.016722e+00 -2.119133e-01   \n",
       "std    8.077749e+00  4.925550e+01  2.373298e+00  8.517471e-01  9.609049e-01   \n",
       "min    1.000000e+00  1.100000e+01  1.000000e+00  0.000000e+00 -1.255300e+00   \n",
       "25%    7.750000e+00  5.400000e+01  2.000000e+00  0.000000e+00 -1.255300e+00   \n",
       "50%    1.450000e+01  9.300000e+01  4.000000e+00  1.000000e+00 -3.010300e-01   \n",
       "75%    2.125000e+01  1.380000e+02  6.000000e+00  2.000000e+00  6.989700e-01   \n",
       "max    2.800000e+01  1.970000e+02  1.200000e+01  2.000000e+00  1.255300e+00   \n",
       "\n",
       "               freq        length       MaxJump     bad_epoch      stopword  \\\n",
       "count  2.586948e+06  2.586948e+06  2.586948e+06  2.586948e+06  2.586948e+06   \n",
       "mean   4.377367e+04  4.341137e+00  3.199554e-01  6.005733e-01  4.760312e-01   \n",
       "std    6.940815e+04  2.360397e+00  1.217291e+00  4.897807e-01  4.994253e-01   \n",
       "min    0.000000e+00  1.000000e+00 -1.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    1.940000e+02  2.000000e+00 -1.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    2.718000e+03  4.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00   \n",
       "75%    6.221400e+04  6.000000e+00  2.000000e+00  1.000000e+00  1.000000e+00   \n",
       "max    2.647210e+05  1.200000e+01  2.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "       ...          E119          E120          E121          E122  \\\n",
       "count  ...  2.586948e+06  2.586948e+06  2.586948e+06  2.586948e+06   \n",
       "mean   ... -9.378470e-02 -3.066079e-03 -4.883013e-02  1.872346e-02   \n",
       "std    ...  1.199427e+01  1.287864e+01  1.201152e+01  1.230401e+01   \n",
       "min    ... -5.808102e+02 -7.236652e+02 -7.258200e+02 -7.262158e+02   \n",
       "25%    ... -6.574130e+00 -6.429716e+00 -6.499453e+00 -6.574852e+00   \n",
       "50%    ... -1.456375e-01 -1.038615e-01 -8.447389e-02 -2.715446e-02   \n",
       "75%    ...  6.298971e+00  6.258519e+00  6.357286e+00  6.577438e+00   \n",
       "max    ...  1.967297e+02  1.282204e+03  1.885576e+02  2.219662e+02   \n",
       "\n",
       "               E123          E124          E125          E126          E127  \\\n",
       "count  2.586948e+06  2.586948e+06  2.586948e+06  2.586948e+06  2.586948e+06   \n",
       "mean  -4.121687e-02 -4.585046e-02  4.162723e-02  1.097454e-01  1.410374e-01   \n",
       "std    1.280992e+01  1.357826e+01  1.460180e+01  1.268019e+01  1.212173e+01   \n",
       "min   -7.530132e+02 -7.490185e+02 -7.527536e+02 -7.541962e+02 -7.518157e+02   \n",
       "25%   -6.698232e+00 -6.586955e+00 -7.274955e+00 -6.762504e+00 -6.425458e+00   \n",
       "50%   -3.462234e-02 -1.620925e-02 -3.477534e-03  3.852869e-02  9.089095e-02   \n",
       "75%    6.600107e+00  6.479055e+00  7.341053e+00  6.925016e+00  6.657640e+00   \n",
       "max    1.895408e+02  3.634467e+02  2.147726e+02  1.707255e+02  1.692756e+02   \n",
       "\n",
       "               E128  \n",
       "count  2.586947e+06  \n",
       "mean   1.750807e-01  \n",
       "std    1.277105e+01  \n",
       "min   -7.520677e+02  \n",
       "25%   -6.377016e+00  \n",
       "50%    8.002501e-02  \n",
       "75%    6.611491e+00  \n",
       "max    2.543207e+02  \n",
       "\n",
       "[8 rows x 138 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La picadura de ciertas ara√±as puede ser mortal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cuando hay hambre no hay pan duro.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La pel√≠cula termin√≥ de forma extra√±a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El gato atrap√≥ muchos ratones.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sobre gustos no hay nada escrito.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentences\n",
       "0  La picadura de ciertas ara√±as puede ser mortal.\n",
       "1               Cuando hay hambre no hay pan duro.\n",
       "2            La pel√≠cula termin√≥ de forma extra√±a.\n",
       "3                   El gato atrap√≥ muchos ratones.\n",
       "4                Sobre gustos no hay nada escrito."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Va a reconstruir los frases base en la apracion punta\n",
    "frase=\"\"\n",
    "sents=[]\n",
    "for word in df.loc[df.suj_id==1][' pal'].to_list():\n",
    "    if frase==\"\":\n",
    "        frase=word\n",
    "    elif word.find(\".\")>-1:\n",
    "        frase=frase+\" \"+word\n",
    "        sents.append(frase)\n",
    "        frase=\"\"\n",
    "    else:\n",
    "        frase=frase+\" \"+word\n",
    "\n",
    "df_phrases=pd.DataFrame({\"sentences\":sents})\n",
    "df_phrases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda a csv\n",
    "# df_phrases.to_csv(\"sentences.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Va a traer un parte del df\n",
    "df_subset=df[\n",
    "    [\n",
    "        'suj_id',\n",
    "        ' pal',\n",
    "        ' palnum',\n",
    "        ' freq',\n",
    "        ' length',\n",
    "        ' time',\n",
    "    ]\n",
    "].copy()\n",
    "df_subset.columns=[\"suj_id\", \"pal\", \"palnum\", \"freq\", \"length\", \"time\"]\n",
    "df_subset.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2586948it [00:47, 54218.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Va a identificar frases base en un punto aparicion\n",
    "# No va a identificar el mismo frase con el mismo identificacion de subjetos diferentes.\n",
    "sent_grp_count=0\n",
    "sent_id=[]\n",
    "for i,row in tqdm(df_subset.iterrows()):\n",
    "     sent_id.append(sent_grp_count)\n",
    "     word=row.pal\n",
    "     word_period=word.find(\".\")\n",
    "     if word_period>-1:\n",
    "          sent_grp_count+=1\n",
    "\n",
    "df_subset['frase_id']=sent_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras esta ayudable que lo ultima palabra tiene un punto, de hecho quieria en una forma que lo esta separado. Por ejemplo:\n",
    "\n",
    "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
    "\n",
    "En esta forma, podemos usar NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92391it [00:02, 44374.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Agarro el primer grupo de frases. Ni importa que lo agarras\n",
    "lectura=[]\n",
    "lectura_frases=[] \n",
    "prev_frase_id=0\n",
    "for i,row in tqdm(df_subset.loc[df_subset.suj_id==df_subset.suj_id[0]].iterrows()):\n",
    "    # Verificar si la proxima palabra esta dentro mismo frase de la ultima palabra\n",
    "    if row.frase_id==prev_frase_id:\n",
    "        lectura.append(row.pal)\n",
    "\n",
    "    # Si no, replecar el punto de la ultima palabra.\n",
    "    # Append el punto a la lista de palabras\n",
    "    # Append la lista a la lista de frases\n",
    "    elif row.frase_id!=prev_frase_id:\n",
    "        ult_pal=lectura[-1]\n",
    "        ult_pal=ult_pal.replace(\".\", \"\")\n",
    "        lectura[-1]=ult_pal\n",
    "        lectura.append(\".\")\n",
    "        lectura_frases.append(lectura)\n",
    "        \n",
    "        prev_frase_id=row.frase_id\n",
    "        lectura=[]\n",
    "        lectura.append(row.pal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No', 'te', 'des', 'por', 'vencido', 'ni', 'a√∫n', 'vencido', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifica la ultima elemento de la lista de frases\n",
    "lectura_frases[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una manera que es mas facil es por NLTK `word_tokenize`.\n",
    "\n",
    "Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La', 'picadura', 'de', 'ciertas', 'ara√±as', 'puede', 'ser', 'mortal', '.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La picadura de ciertas ara√±as puede ser mortal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cuando hay hambre no hay pan duro.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La pel√≠cula termin√≥ de forma extra√±a.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentences\n",
       "0  La picadura de ciertas ara√±as puede ser mortal.\n",
       "1               Cuando hay hambre no hay pan duro.\n",
       "2            La pel√≠cula termin√≥ de forma extra√±a."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\n",
    "    word_tokenize(df_phrases.sentences[0], language=\"spanish\")\n",
    ")\n",
    "df_phrases.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12360it [00:06, 2014.02it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized=[]\n",
    "bigram=[]\n",
    "bigram_pos_tag=[]\n",
    "for i,row in tqdm(df_phrases.iterrows()):\n",
    "    tokenized.append(word_tokenize(row.sentences, language=\"spanish\"))\n",
    "    bigrams=ngrams(tokenized[-1], 2)\n",
    "    bigram_pos_tag.append(nltk.pos_tag(tokenized[-1]))\n",
    "    bigram.append(list(bigrams))\n",
    "df_phrases[\"sentences_tokenized\"]=tokenized\n",
    "df_phrases[\"bigrams\"]=bigram\n",
    "df_phrases[\"bigrams_pos_tag\"]=bigram_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentences_tokenized</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>bigrams_pos_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La picadura de ciertas ara√±as puede ser mortal.</td>\n",
       "      <td>[La, picadura, de, ciertas, ara√±as, puede, ser...</td>\n",
       "      <td>[(La, picadura), (picadura, de), (de, ciertas)...</td>\n",
       "      <td>[(La, NNP), (picadura, FW), (de, FW), (ciertas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cuando hay hambre no hay pan duro.</td>\n",
       "      <td>[Cuando, hay, hambre, no, hay, pan, duro, .]</td>\n",
       "      <td>[(Cuando, hay), (hay, hambre), (hambre, no), (...</td>\n",
       "      <td>[(Cuando, NNP), (hay, NN), (hambre, NN), (no, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La pel√≠cula termin√≥ de forma extra√±a.</td>\n",
       "      <td>[La, pel√≠cula, termin√≥, de, forma, extra√±a, .]</td>\n",
       "      <td>[(La, pel√≠cula), (pel√≠cula, termin√≥), (termin√≥...</td>\n",
       "      <td>[(La, NNP), (pel√≠cula, NN), (termin√≥, NN), (de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         sentences  \\\n",
       "0  La picadura de ciertas ara√±as puede ser mortal.   \n",
       "1               Cuando hay hambre no hay pan duro.   \n",
       "2            La pel√≠cula termin√≥ de forma extra√±a.   \n",
       "\n",
       "                                 sentences_tokenized  \\\n",
       "0  [La, picadura, de, ciertas, ara√±as, puede, ser...   \n",
       "1       [Cuando, hay, hambre, no, hay, pan, duro, .]   \n",
       "2     [La, pel√≠cula, termin√≥, de, forma, extra√±a, .]   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [(La, picadura), (picadura, de), (de, ciertas)...   \n",
       "1  [(Cuando, hay), (hay, hambre), (hambre, no), (...   \n",
       "2  [(La, pel√≠cula), (pel√≠cula, termin√≥), (termin√≥...   \n",
       "\n",
       "                                     bigrams_pos_tag  \n",
       "0  [(La, NNP), (picadura, FW), (de, FW), (ciertas...  \n",
       "1  [(Cuando, NNP), (hay, NN), (hambre, NN), (no, ...  \n",
       "2  [(La, NNP), (pel√≠cula, NN), (termin√≥, NN), (de...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_phrases.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(La, NNP), (picadura, FW), (de, FW), (ciertas...\n",
       "1        [(Cuando, NNP), (hay, NN), (hambre, NN), (no, ...\n",
       "2        [(La, NNP), (pel√≠cula, NN), (termin√≥, NN), (de...\n",
       "3        [(El, NNP), (gato, NN), (atrap√≥, NN), (muchos,...\n",
       "4        [(Sobre, NNP), (gustos, VBZ), (no, DT), (hay, ...\n",
       "                               ...                        \n",
       "12355    [(No, DT), (hagas, NN), (promesas, NN), (que, ...\n",
       "12356    [(El, NNP), (que, VBZ), (a, DT), (hierro, NN),...\n",
       "12357    [(Los, NNP), (loros, JJ), (comieron, NN), (la,...\n",
       "12358    [(No, DT), (te, NN), (des, VBZ), (por, JJ), (v...\n",
       "12359    [(Lucifer, NNP), (es, JJ), (uno, NN), (de, IN)...\n",
       "Name: bigrams_pos_tag, Length: 12360, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_phrases.bigrams_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the unigram tagger\n",
    "# uni_tag = ut(cess_sents)\n",
    "# uni_tag.tag(df_phrases.bigrams[0])\n",
    "nltk.pos_tag(df_phrases.bigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricit√©_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunci√≥', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_√Åguila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japon√©s', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')], [('Una', 'di0fs0'), ('portavoz', 'nccs000'), ('de', 'sps00'), ('EDF', 'np00000'), ('explic√≥', 'vmis3s0'), ('a', 'sps00'), ('EFE', 'np00000'), ('que', 'cs'), ('el', 'da0ms0'), ('proyecto', 'ncms000'), ('para', 'sps00'), ('la', 'da0fs0'), ('construcci√≥n', 'ncfs000'), ('de', 'sps00'), ('Altamira_2', 'np00000'), (',', 'Fc'), ('al', 'spcms'), ('norte', 'ncms000'), ('de', 'sps00'), ('Tampico', 'np00000'), (',', 'Fc'), ('prev√©', 'vmm02s0'), ('la', 'da0fs0'), ('utilizaci√≥n', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('como', 'cs'), ('combustible', 'ncms000'), ('principal', 'aq0cs0'), ('en', 'sps00'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('ciclo', 'ncms000'), ('combinado', 'aq0msp'), ('que', 'pr0cn000'), ('debe', 'vmip3s0'), ('empezar', 'vmn0000'), ('a', 'sps00'), ('funcionar', 'vmn0000'), ('en', 'sps00'), ('mayo_del_2002', 'W'), ('.', 'Fp')], ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cess_sents[:train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.838390230012339"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split corpus into training and testing set.\n",
    "train = int(len(df_phrases.bigrams_pos_tag)*70/100) # 90%\n",
    "\n",
    "# # Train a bigram tagger with only training data.\n",
    "bi_tag = bt(df_phrases.iloc[:train].bigrams_pos_tag.to_list())\n",
    "\n",
    "# # Evaluates on testing data remaining 10%\n",
    "bi_tag.accuracy(df_phrases.iloc[train+1:].bigrams_pos_tag.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('La', 'picadura'), None),\n",
       " (('picadura', 'de'), None),\n",
       " (('de', 'ciertas'), None),\n",
       " (('ciertas', 'ara√±as'), None),\n",
       " (('ara√±as', 'puede'), None),\n",
       " (('puede', 'ser'), None),\n",
       " (('ser', 'mortal'), None),\n",
       " (('mortal', '.'), None)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_tag.tag(df_phrases.bigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Various', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('apartments', 'NNS'),\n",
       " ('are', 'BER'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('terrace', 'NN'),\n",
       " ('type', 'NN'),\n",
       " (',', ','),\n",
       " ('being', 'BEG'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('ground', 'NN'),\n",
       " ('floor', 'NN'),\n",
       " ('so', 'QL'),\n",
       " ('that', 'CS'),\n",
       " ('entrance', 'NN'),\n",
       " ('is', 'BEZ'),\n",
       " ('direct', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "unigram_tagger.tag(brown_sents[2007])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El', 'grupo', 'estatal', 'Electricit√©_de_France', ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.cess_esp.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4623\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n"
     ]
    }
   ],
   "source": [
    "print(len(brown_sents))\n",
    "print(brown_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suj_id</th>\n",
       "      <th>pal</th>\n",
       "      <th>palnum</th>\n",
       "      <th>freq</th>\n",
       "      <th>length</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>La</td>\n",
       "      <td>1</td>\n",
       "      <td>192476</td>\n",
       "      <td>2</td>\n",
       "      <td>-101.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>picadura</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>-101.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>de</td>\n",
       "      <td>3</td>\n",
       "      <td>264721</td>\n",
       "      <td>2</td>\n",
       "      <td>-101.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ciertas</td>\n",
       "      <td>4</td>\n",
       "      <td>380</td>\n",
       "      <td>7</td>\n",
       "      <td>-101.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ara√±as</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>-101.5625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25111</th>\n",
       "      <td>28</td>\n",
       "      <td>la</td>\n",
       "      <td>5</td>\n",
       "      <td>192476</td>\n",
       "      <td>2</td>\n",
       "      <td>664.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25112</th>\n",
       "      <td>28</td>\n",
       "      <td>panader√≠a</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>664.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25113</th>\n",
       "      <td>28</td>\n",
       "      <td>cocinan</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>664.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25114</th>\n",
       "      <td>28</td>\n",
       "      <td>el</td>\n",
       "      <td>8</td>\n",
       "      <td>139594</td>\n",
       "      <td>2</td>\n",
       "      <td>664.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25115</th>\n",
       "      <td>28</td>\n",
       "      <td>pan.</td>\n",
       "      <td>9</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "      <td>664.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2586948 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       suj_id        pal  palnum    freq  length      time\n",
       "0           1         La       1  192476       2 -101.5625\n",
       "1           1   picadura       2       9       8 -101.5625\n",
       "2           1         de       3  264721       2 -101.5625\n",
       "3           1    ciertas       4     380       7 -101.5625\n",
       "4           1     ara√±as       5      20       6 -101.5625\n",
       "...       ...        ...     ...     ...     ...       ...\n",
       "25111      28         la       5  192476       2  664.0625\n",
       "25112      28  panader√≠a       6      10       9  664.0625\n",
       "25113      28    cocinan       7       7       7  664.0625\n",
       "25114      28         el       8  139594       2  664.0625\n",
       "25115      28       pan.       9     306       4  664.0625\n",
       "\n",
       "[2586948 rows x 6 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset.groupby(\n",
    "    [\"suj_id\", \"\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-general-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
